from imutils.video import VideoStream
import face_recognition
import imutils
import pickle
from pathlib import Path
import cv2
import pyttsx3
import os
import time
from itertools import groupby
from imutils import paths
from ntpath import basename
from multiprocessing import Process


class Recognizer():
    """
    A class for initializing and running the facial recognition software.

    ...

    Attributes
    ----------
    [There are currently no attributes of this class.]

    Methods
    -------
    run(min_area=500, wait=2, max_photos=5)
        Starts video stream and motion detector. Once motion is detected, checks for and examines faces in video frames.
        Captures images of both known and unknown faces and trains algorithm on new data.
    """

    def __init__(self):
        return

    def init_stream(self, t=2):
        """
        Starts video stream.

        Parameters
        ----------
        t : int, optional
            Delay before video stream begins (default is 2 seconds)

        Returns
        -------
        vs : VideoStream object
        """

        vs = VideoStream(src=0).start()
        time.sleep(t)
        return vs

    def detect_motion(self, first_frame, curr_frame, min_area):
        """
        Detects motion by comparing differences in video frames.

        Parameters
        ----------
        first_frame : VideoStream frame
            Base frame used to compare with current frame. First frame captured either (a) after video stream beings or
            (b) face is recognized
        curr_frame : VideoStream frame
            The current frame in the video stream
        min_area : int
            The minimum area in pixels of the difference between first_frame and curr_frame necessary to trigger motion
            detection

        Returns
        -------
        True if motion is detected, otherwise False
        """

        # Detects and highlights differences between frames
        frame_delta = cv2.absdiff(first_frame, curr_frame)
        threshold = cv2.threshold(frame_delta, 25, 225, cv2.THRESH_BINARY)[1]
        threshold = cv2.dilate(threshold, None, iterations=2)
        contours = cv2.findContours(threshold.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        contours = imutils.grab_contours(contours)

        # Checks if highlighted differences are greater than threshold for detection
        for c in contours:
            if cv2.contourArea(c) > min_area:
                return True
        return False

    def open_encodings(self, path):
        """
        Open facial encodings from pickle file.

        Parameters
        ----------
        path : str
            Location of pickle file

        Returns
        -------
        data : JSON
            Unpickled data as JSON
        """

        data = pickle.loads(open(path, "rb").read())
        return data

    def recognize_faces(self, frame, data):
        """
        Detects and recognizes faces in a video frame.

        Parameters
        ----------
        frame : VideoStream frame
            Candidate frame for facial recognition
        data : JSON
            Facial encodings generated by facial_recognition.facial_encodings function

        Returns
        -------
        names : list
            List of names of recognized faces
        """

        # Generates encodings for all detected faces
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        rgb = imutils.resize(rgb, width=750)
        boxes = face_recognition.face_locations(rgb, model="hog")
        encodings = face_recognition.face_encodings(rgb, boxes)
        names = []

        # Checks if generated encodings match known encodings. If so, saves the name of detected individuals. If not,
        # marks face as 'unknown'
        for encoding in encodings:
            name = "unknown"
            matches = face_recognition.compare_faces(data["encodings"], encoding)
            if True in matches:
                matched_idxs = [i for (i, b) in enumerate(matches) if b]
                counts = {}
                for i in matched_idxs:
                    name = data["names"][i]
                    counts[name] = counts.get(name, 0) + 1
                name = max(counts, key=counts.get)
            names.append(name)
        return names

    def list_to_string(self, l):
        """
        Converts list of items to string.

        Output string formatted as "Hello [item1] and [item2] and ... and [item9]!"

        Parameters
        ----------
        l : list
            List of names

        Returns
        -------
        s : str
            String to be fed into text-to-speech operator
        """

        s = 'Hello '
        for item in l:
            if item != l[-1]:
                item = str(item).replace('_', ' ')
                s += item
                s += " and "
            else:
                item = str(item).replace('_', ' ')
                s += item
                s += "!"
        return s

    def round_down(self, num, divisor):
        """
        Helper function to round number down to nearest multiple of x

        Parameters
        ----------
        num : int
            Number to be rounded down
        divisor : int
            Value to be round to nearest multiple of

        Returns
        -------
        Num rounded down to nearest multiple of divisor
        """

        return num - (num % divisor)

    def clean_up_photos(self, path, batch_sep=300, keep=2):
        """
        Removes additional training photos from an individual's directory

        Separates all photos of an individual into batches based on a time interval and deletes all but the newest n

        Parameters
        ----------
        path : str
            Location of all photos of an individual
        batch_sep : int, optional
            Interval in seconds between batches of photos (default is 300)
        keep : int, optional
            Number of photos to keep in each batch (default is 2)

        Returns
        -------
        Does not return a value
        """

        files = sorted(os.listdir(path))

        # Batches photos according to time captured and deletes all but n (default = 2) of them
        for key, group in groupby(files, lambda x: self.round_down(int(float(Path(x).stem)), batch_sep)):
            for file in list(group)[:-keep]:
                print(file)
                os.remove(f"{path}/{file}")
        return

    def capture_data(self, frame, output_paths):
        """
        Captures a photo and saves it to directory(ies) associated with individuals in the photo

        Parameters
        ----------
        frame : VideoStream frame
            The photo to save
        output_paths : list
            Paths to the locations of all people recognized in the frame
        max_photos : int, optional
            Maximum number of photos saved to an individual's directory before calling clean_up_photos (default is 20)

        Returns
        -------
        Does not return a value
        """

        for path in output_paths:

            # Checks that path to directory exists. If not, creates directory.
            try:
                os.makedirs(path)
            except FileExistsError:
                pass
            p = os.path.sep.join([path, "n_{}.png".format(str(time.time()))])

            # Saves photo to appropriate directory
            cv2.imwrite(p, frame)
        print("Captured photo.")
        return

    def train_alg(self, path_to_data):
        # grab the paths to the input images in our dataset
        print("[INFO] quantifying faces...")
        image_paths = [i for i in list(paths.list_images(path_to_data)) if basename(i)[0] == "n"]
        print(image_paths)

        # initialize the list of known encodings and known names
        known_encodings = []
        known_names = []

        # loop over the image paths
        for (i, image_path) in enumerate(image_paths):
            # extract the person name from the image path
            print("[INFO] processing image {}/{}".format(i + 1, len(image_paths)))
            name = image_path.split(os.path.sep)[-2]

            # load the input image and convert it from BGR (OpenCV ordering)
            # to dlib ordering (RGB)
            image = cv2.imread(image_path)
            rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # detect the (x, y)-coordinates of the bounding boxes
            # corresponding to each face in the input image
            boxes = face_recognition.face_locations(rgb, model="hog")

            # compute the facial embedding for the face
            encodings = face_recognition.face_encodings(rgb, boxes)

            # loop over the encodings
            for encoding in encodings:
                # add each encoding + name to our set of known names and
                # encodings
                known_encodings.append(encoding)
                known_names.append(name)

            renamed = "/".join(image_path.split(os.path.sep)[:-1]) + "/" + image_path.split(os.path.sep)[-1][2:]
            print(renamed)
            os.rename(image_path, renamed)

        # TODO: make sure that data is being edited and stored properly and functionally
        data = self.open_encodings("/users/abel/desktop/face_recognition/encodings.pickle")
        data["encodings"].extend(known_encodings)
        data["names"].extend(known_names)

        # dump the facial encodings + names to disk
        print("[INFO] serializing encodings...")
        f = open("/users/abel/desktop/face_recognition/encodings.pickle", "wb")
        f.write(pickle.dumps(data))
        f.close()

        # TODO: handle photo deletion if directories are bloated
        '''if len(os.listdir(path)) > max_photos:
            self.clean_up_photos(path)'''
        return

    def run(self, min_area=500, wait=2, max_photos=5):
        """
        Runs the motion detection and facial recognition loop.

        This function controls the overall operation of the facial detection program. It waits until motion is detected
        in the video stream, then looks for faces in frames where motion has been detected. If known faces are detected,
        the program says, "Hello [person]", and captures an additional n photos of them. These photos are saved to a
        directory named after that person. If no known faces are detected, the program saves unknown faces to a new
        directory.

        Parameters
        ----------
        min_area : int, optional
            The minimum difference between frames neccessary to trigger motion detection (default is 500)
        wait : int, optional
            Wait time before video stream commences (default is 2 seconds)
        max_photos : int, optional
            The maximum number of photos the program takes of a detected face (default is 5)

        Returns
        -------
        data : JSON
            Unpickled data as JSON
        """

        # Initializes text-to-speak engine, opens facial encoding data, initializes video stream, and sets flags for
        # motion detection, recognized faces and photos captured.
        engine = pyttsx3.init()
        data = self.open_encodings("encodings.pickle")
        vs = self.init_stream(wait)
        print("Looking for motion.")
        empty = True
        cur_faces = []
        photo_counter = max_photos

        # Begins video stream loop
        while True:
            # Sets motion detection base frame to None
            first_frame = None
            no_prior_motion = False

            # Checks frames for motion detection
            while empty:
                # TODO: run separate process for training algorithm

                frame = vs.read()
                frame = imutils.resize(frame, width=500)
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                blur = cv2.GaussianBlur(gray, (21, 21), 0)
                if first_frame is None:
                    first_frame = blur
                    no_prior_motion = True
                    continue
                if self.detect_motion(first_frame, blur, min_area):
                    if no_prior_motion:
                        empty = False
                else:
                    no_prior_motion = True

            # When motion is detected, searches frames for faces
            while not empty:
                frame = vs.read()
                names = self.recognize_faces(frame, data)
                # If at least one face is detected and fewer than max_photos have been taken, take a photo
                if photo_counter < max_photos and names:
                    n = [str(int(time.time())) if name == "unknown" else name for name in names]
                    output_paths = [f"dataset/{name}" for name in n]
                    self.capture_data(frame, output_paths)
                    photo_counter += 1
                # If no faces differ from previous frame, break out of facial detection loop
                if set(names) == set(cur_faces):
                    empty = True
                    break
                # If no faces are detected, check next frame
                if not names:
                    continue
                cur_faces = names
                photo_counter = 0
                to_speak = self.list_to_string(names)
                # If only one person is detected and unknown, ask who they are
                if "unknown" in names and len(names) == 1:
                    print("Who dat?")
                    engine.say("I don't know who you are...")
                    engine.runAndWait()
                    empty = True
                # Otherwise, say hello to recognized faces
                else:
                    print(to_speak)
                    engine.say(to_speak)
                    engine.runAndWait()
                    empty = True
            #print("Face detected. Looking for motion.")
        return
